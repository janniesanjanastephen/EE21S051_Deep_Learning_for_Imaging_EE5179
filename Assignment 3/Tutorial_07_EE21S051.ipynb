{"cells":[{"cell_type":"markdown","metadata":{"id":"XAjZsNemiGY2"},"source":["# 2 - Improving Performance\n","\n","In the previous notebook, we got the fundamentals down for sentiment analysis. In this notebook, we'll actually get decent results.\n","\n","We will use:\n","- bidirectional RNN\n","- multi-layer RNN\n","\n","This will allow us to achieve ~84% test accuracy."]},{"cell_type":"markdown","metadata":{"id":"3c53mzb7iGY7"},"source":["## Preparing Data"]},{"cell_type":"code","execution_count":75,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z3YrJix0ipF8","outputId":"38ffeea4-20f0-4dd2-8126-4f86e3c97f22","executionInfo":{"status":"ok","timestamp":1664789103532,"user_tz":-330,"elapsed":4626,"user":{"displayName":"Jannie Sanjana Stephen","userId":"11861105524006828101"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchtext==0.10.0 in /usr/local/lib/python3.7/dist-packages (0.10.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (4.64.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (2.23.0)\n","Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (1.9.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (1.21.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchtext==0.10.0) (4.1.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2.10)\n"]}],"source":["!pip install torchtext==0.10.0"]},{"cell_type":"code","execution_count":76,"metadata":{"id":"nnAhxOmniGY8","executionInfo":{"status":"ok","timestamp":1664789104106,"user_tz":-330,"elapsed":585,"user":{"displayName":"Jannie Sanjana Stephen","userId":"11861105524006828101"}}},"outputs":[],"source":["import torch\n","from torchtext.legacy import data\n","\n","SEED = 1234\n","\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","TEXT = data.Field(tokenize = 'spacy',\n","                  tokenizer_language = 'en_core_web_sm')\n","\n","LABEL = data.LabelField(dtype = torch.float)"]},{"cell_type":"code","execution_count":77,"metadata":{"id":"lCnFCkp8iGY_","executionInfo":{"status":"ok","timestamp":1664789151560,"user_tz":-330,"elapsed":47465,"user":{"displayName":"Jannie Sanjana Stephen","userId":"11861105524006828101"}}},"outputs":[],"source":["from torchtext.legacy import datasets\n","\n","train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"]},{"cell_type":"code","execution_count":78,"metadata":{"id":"n7Mgh59EiGZA","executionInfo":{"status":"ok","timestamp":1664789151563,"user_tz":-330,"elapsed":59,"user":{"displayName":"Jannie Sanjana Stephen","userId":"11861105524006828101"}}},"outputs":[],"source":["import random\n","\n","train_data, valid_data = train_data.split(random_state = random.seed(SEED))"]},{"cell_type":"markdown","metadata":{"id":"qI5sKl_JiGZB"},"source":["Next is the use of pre-trained word embeddings. Now, instead of having our word embeddings initialized randomly, they are initialized with these pre-trained vectors.\n","We get these vectors simply by specifying which vectors we want and passing it as an argument to `build_vocab`. `TorchText` handles downloading the vectors and associating them with the correct words in our vocabulary.\n","\n","Here, we'll be using the `\"glove.6B.100d\" vectors\"`. `glove` is the algorithm used to calculate the vectors, go [here](https://nlp.stanford.edu/projects/glove/) for more. `6B` indicates these vectors were trained on 6 billion tokens and `100d` indicates these vectors are 100-dimensional.\n","\n","You can see the other available vectors [here](https://github.com/pytorch/text/blob/master/torchtext/vocab.py#L113).\n","\n","The theory is that these pre-trained vectors already have words with similar semantic meaning close together in vector space, e.g. \"terrible\", \"awful\", \"dreadful\" are nearby. This gives our embedding layer a good initialization as it does not have to learn these relations from scratch."]},{"cell_type":"code","execution_count":79,"metadata":{"id":"uUiMbHqDiGZC","executionInfo":{"status":"ok","timestamp":1664789155128,"user_tz":-330,"elapsed":3615,"user":{"displayName":"Jannie Sanjana Stephen","userId":"11861105524006828101"}}},"outputs":[],"source":["MAX_VOCAB_SIZE = 25_000\n","\n","TEXT.build_vocab(train_data, \n","                 max_size = MAX_VOCAB_SIZE, \n","                 vectors = \"glove.6B.100d\", \n","                 unk_init = torch.Tensor.normal_)\n","\n","LABEL.build_vocab(train_data)"]},{"cell_type":"code","execution_count":80,"metadata":{"id":"0j9QLueUiGZE","executionInfo":{"status":"ok","timestamp":1664789155131,"user_tz":-330,"elapsed":71,"user":{"displayName":"Jannie Sanjana Stephen","userId":"11861105524006828101"}}},"outputs":[],"source":["BATCH_SIZE = 64\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n","    (train_data, valid_data, test_data), \n","    batch_size = BATCH_SIZE,\n","    sort_within_batch = True,\n","    device = device)"]},{"cell_type":"markdown","metadata":{"id":"Ypnhdw1JiGZE"},"source":["## Build the Model\n","\n","### Different RNN Architecture\n","\n","We'll be using a different RNN architecture called a Long Short-Term Memory (LSTM). Why is an LSTM better than a standard RNN? Standard RNNs suffer from the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem). LSTMs overcome this by having an extra recurrent state called a _cell_, $c$ - which can be thought of as the \"memory\" of the LSTM - and the use use multiple _gates_ which control the flow of information into and out of the memory. For more information, go [here](https://colah.github.io/posts/2015-08-Understanding-LSTMs/). We can simply think of the LSTM as a function of $x_t$, $h_t$ and $c_t$, instead of just $x_t$ and $h_t$.\n","\n","$$(h_t, c_t) = \\text{LSTM}(x_t, h_t, c_t)$$\n","\n","Thus, the model using an LSTM looks something like (with the embedding layers omitted):\n","\n","![](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/assets/sentiment2.png?raw=1)\n","\n","The initial cell state, $c_0$, like the initial hidden state is initialized to a tensor of all zeros. The sentiment prediction is still, however, only made using the final hidden state, not the final cell state, i.e. $\\hat{y}=f(h_T)$.\n","\n","### Bidirectional RNN\n","\n","The concept behind a bidirectional RNN is simple. As well as having an RNN processing the words in the sentence from the first to the last (a forward RNN), we have a second RNN processing the words in the sentence from the **last to the first** (a backward RNN). At time step $t$, the forward RNN is processing word $x_t$, and the backward RNN is processing word $x_{T-t+1}$. \n","\n","In PyTorch, the hidden state (and cell state) tensors returned by the forward and backward RNNs are stacked on top of each other in a single tensor. \n","\n","We make our sentiment prediction using a concatenation of the last hidden state from the forward RNN (obtained from final word of the sentence), $h_T^\\rightarrow$, and the last hidden state from the backward RNN (obtained from the first word of the sentence), $h_T^\\leftarrow$, i.e. $\\hat{y}=f(h_T^\\rightarrow, h_T^\\leftarrow)$   \n","\n","The image below shows a bi-directional RNN, with the forward RNN in orange, the backward RNN in green and the linear layer in silver.  \n","\n","![](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/assets/sentiment3.png?raw=1)\n","\n","### Multi-layer RNN\n","\n","Multi-layer RNNs (also called *deep RNNs*) are another simple concept. The idea is that we add additional RNNs on top of the initial standard RNN, where each RNN added is another *layer*. The hidden state output by the first (bottom) RNN at time-step $t$ will be the input to the RNN above it at time step $t$. The prediction is then made from the final hidden state of the final (highest) layer.\n","\n","The image below shows a multi-layer unidirectional RNN, where the layer number is given as a superscript. Also note that each layer needs their own initial hidden state, $h_0^L$.\n","\n","![](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/assets/sentiment4.png?raw=1)\n"]},{"cell_type":"code","execution_count":112,"metadata":{"id":"nLvXVA7OiGZF","executionInfo":{"status":"ok","timestamp":1664789821404,"user_tz":-330,"elapsed":320,"user":{"displayName":"Jannie Sanjana Stephen","userId":"11861105524006828101"}}},"outputs":[],"source":["import torch.nn as nn\n","\n","class LSTM(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n","                 bidirectional, dropout, pad_idx):\n","        \n","        super().__init__()\n","        \n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n","        self.rnn = nn.LSTM(embedding_dim, \n","                           hidden_dim, \n","                           num_layers=n_layers, \n","                           bidirectional=bidirectional, \n","                           dropout=dropout)\n","        \n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, text):\n","        \n","        #text = [sent len, batch size]\n","        \n","        embedded = self.dropout(self.embedding(text))\n","        \n","        #embedded = [sent len, batch size, emb dim]\n","        \n","        output, (hidden, cell) = self.rnn(embedded)\n","\n","        #output = [sent len, batch size, hid dim * num directions]\n","        #output over padding tokens are zero tensors\n","        \n","        #hidden = [num layers * num directions, batch size, hid dim]\n","        #cell = [num layers * num directions, batch size, hid dim]\n","        \n","        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n","        #and apply dropout\n","        \n","        hidden = self.dropout(hidden[-1,:,:])\n","                \n","        #hidden = [batch size, hid dim * num directions]\n","            \n","        return self.fc(hidden)"]},{"cell_type":"code","execution_count":113,"metadata":{"id":"Ad1sTsNviGZG","executionInfo":{"status":"ok","timestamp":1664789824170,"user_tz":-330,"elapsed":308,"user":{"displayName":"Jannie Sanjana Stephen","userId":"11861105524006828101"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b6adf3d6-00eb-4dde-f95e-ac6635ebba50"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"]}],"source":["INPUT_DIM = len(TEXT.vocab)\n","EMBEDDING_DIM = 100\n","HIDDEN_DIM = 256\n","OUTPUT_DIM = 1\n","N_LAYERS = 1\n","BIDIRECTIONAL = False\n","DROPOUT = 0.5\n","PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n","\n","model = LSTM(INPUT_DIM, \n","            EMBEDDING_DIM, \n","            HIDDEN_DIM, \n","            OUTPUT_DIM, \n","            N_LAYERS, \n","            BIDIRECTIONAL, \n","            DROPOUT, \n","            PAD_IDX)"]},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class Bidirectional_RNN(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n","                 bidirectional, dropout, pad_idx):\n","        \n","        super().__init__()\n","        \n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n","        self.rnn = nn.LSTM(embedding_dim, \n","                           hidden_dim, \n","                           num_layers=n_layers, \n","                           bidirectional=bidirectional, \n","                           dropout=dropout)\n","        \n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, text):\n","        \n","        #text = [sent len, batch size]\n","        \n","        embedded = self.dropout(self.embedding(text))\n","        \n","        #embedded = [sent len, batch size, emb dim]\n","        \n","        output, (hidden, cell) = self.rnn(embedded)\n","\n","        #output = [sent len, batch size, hid dim * num directions]\n","        #output over padding tokens are zero tensors\n","        \n","        #hidden = [num layers * num directions, batch size, hid dim]\n","        #cell = [num layers * num directions, batch size, hid dim]\n","        \n","        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n","        #and apply dropout\n","        \n","        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n","                \n","        #hidden = [batch size, hid dim * num directions]\n","            \n","        return self.fc(hidden)"],"metadata":{"id":"ibkYOVLHKvQw","executionInfo":{"status":"ok","timestamp":1664789496351,"user_tz":-330,"elapsed":339,"user":{"displayName":"Jannie Sanjana Stephen","userId":"11861105524006828101"}}},"execution_count":96,"outputs":[]},{"cell_type":"code","source":["INPUT_DIM = len(TEXT.vocab)\n","EMBEDDING_DIM = 100\n","HIDDEN_DIM = 256\n","OUTPUT_DIM = 1\n","N_LAYERS = 2\n","BIDIRECTIONAL = True\n","DROPOUT = 0.5\n","PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n","\n","model = Bidirectional_RNN(INPUT_DIM, \n","            EMBEDDING_DIM, \n","            HIDDEN_DIM, \n","            OUTPUT_DIM, \n","            N_LAYERS, \n","            BIDIRECTIONAL, \n","            DROPOUT, \n","            PAD_IDX)"],"metadata":{"id":"wfTOrQboLxPS","executionInfo":{"status":"ok","timestamp":1664789497164,"user_tz":-330,"elapsed":33,"user":{"displayName":"Jannie Sanjana Stephen","userId":"11861105524006828101"}}},"execution_count":97,"outputs":[]},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class Multilayer_RNN(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n","                 bidirectional, dropout, pad_idx):\n","        \n","        super().__init__()\n","        \n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n","        self.rnn = nn.LSTM(embedding_dim, \n","                           hidden_dim, \n","                           num_layers=n_layers, \n","                           bidirectional=bidirectional, \n","                           dropout=dropout)\n","        \n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, text):\n","        \n","        #text = [sent len, batch size]\n","        \n","        embedded = self.dropout(self.embedding(text))\n","        \n","        #embedded = [sent len, batch size, emb dim]\n","        \n","        output, (hidden, cell) = self.rnn(embedded)\n","\n","        #output = [sent len, batch size, hid dim * num directions]\n","        #output over padding tokens are zero tensors\n","        \n","        #hidden = [num layers * num directions, batch size, hid dim]\n","        #cell = [num layers * num directions, batch size, hid dim]\n","        \n","        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n","        #and apply dropout\n","        \n","        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n","                \n","        #hidden = [batch size, hid dim * num directions]\n","            \n","        return self.fc(hidden)"],"metadata":{"id":"2x_jwL9pNao6","executionInfo":{"status":"ok","timestamp":1664790168770,"user_tz":-330,"elapsed":429,"user":{"displayName":"Jannie Sanjana Stephen","userId":"11861105524006828101"}}},"execution_count":126,"outputs":[]},{"cell_type":"code","source":["INPUT_DIM = len(TEXT.vocab)\n","EMBEDDING_DIM = 100\n","HIDDEN_DIM = 256\n","OUTPUT_DIM = 1\n","N_LAYERS = 2\n","BIDIRECTIONAL = False\n","DROPOUT = 0.5\n","PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n","\n","model = Multilayer_RNN(INPUT_DIM, \n","            EMBEDDING_DIM, \n","            HIDDEN_DIM, \n","            OUTPUT_DIM, \n","            N_LAYERS, \n","            BIDIRECTIONAL, \n","            DROPOUT, \n","            PAD_IDX)"],"metadata":{"id":"HLWy33lWNfmz","executionInfo":{"status":"ok","timestamp":1664790173335,"user_tz":-330,"elapsed":331,"user":{"displayName":"Jannie Sanjana Stephen","userId":"11861105524006828101"}}},"execution_count":127,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hNH78tYAiGZH"},"source":["We'll print out the number of parameters in our model. \n","\n","Notice how we have almost twice as many parameters as before!"]},{"cell_type":"code","execution_count":128,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fmbICY8FiGZH","outputId":"0c93b603-0d76-4e12-b725-d22162b19eda","executionInfo":{"status":"ok","timestamp":1664790177362,"user_tz":-330,"elapsed":312,"user":{"displayName":"Jannie Sanjana Stephen","userId":"11861105524006828101"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["The model has 3,393,641 trainable parameters\n"]}],"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"]},{"cell_type":"markdown","metadata":{"id":"-YeLINmbiGZI"},"source":["The final addition is copying the pre-trained word embeddings we loaded earlier into the `embedding` layer of our model.\n","\n","We retrieve the embeddings from the field's vocab, and check they're the correct size, _**[vocab size, embedding dim]**_ "]},{"cell_type":"code","execution_count":129,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C0IGwTZhiGZI","outputId":"300913fe-27c4-44e3-ccf4-8a6e8eee25c1","executionInfo":{"status":"ok","timestamp":1664790180132,"user_tz":-330,"elapsed":317,"user":{"displayName":"Jannie Sanjana Stephen","userId":"11861105524006828101"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([25002, 100])\n"]}],"source":["pretrained_embeddings = TEXT.vocab.vectors\n","\n","print(pretrained_embeddings.shape)"]},{"cell_type":"markdown","metadata":{"id":"jMCPa3MZiGZI"},"source":["We then replace the initial weights of the `embedding` layer with the pre-trained embeddings.\n","\n","**Note**: this should always be done on the `weight.data` and not the `weight`!"]},{"cell_type":"code","execution_count":130,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i5yVKOjPiGZI","outputId":"a3c3af30-1340-45c2-9b2c-c11897c0a916","executionInfo":{"status":"ok","timestamp":1664790183411,"user_tz":-330,"elapsed":523,"user":{"displayName":"Jannie Sanjana Stephen","userId":"11861105524006828101"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n","        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n","        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n","        ...,\n","        [ 0.1068, -0.0572, -0.5956,  ...,  2.1442,  1.2027,  0.3947],\n","        [ 0.3749, -0.0187, -0.3940,  ..., -0.5277,  0.0937, -1.1152],\n","        [ 0.1787,  0.1934, -0.0216,  ..., -0.1655,  0.3625, -0.2256]])"]},"metadata":{},"execution_count":130}],"source":["model.embedding.weight.data.copy_(pretrained_embeddings)"]},{"cell_type":"code","execution_count":131,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KgrxEvGOiGZJ","outputId":"2b049460-5c82-4404-9407-6e49e54994f4","executionInfo":{"status":"ok","timestamp":1664790186218,"user_tz":-330,"elapsed":593,"user":{"displayName":"Jannie Sanjana Stephen","userId":"11861105524006828101"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n","        ...,\n","        [ 0.1068, -0.0572, -0.5956,  ...,  2.1442,  1.2027,  0.3947],\n","        [ 0.3749, -0.0187, -0.3940,  ..., -0.5277,  0.0937, -1.1152],\n","        [ 0.1787,  0.1934, -0.0216,  ..., -0.1655,  0.3625, -0.2256]])\n"]}],"source":["UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n","\n","model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n","model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n","\n","print(model.embedding.weight.data)"]},{"cell_type":"markdown","metadata":{"id":"5t_wnKSNiGZJ"},"source":["## Train the Model"]},{"cell_type":"code","execution_count":132,"metadata":{"id":"qhCC_d2ziGZK","executionInfo":{"status":"ok","timestamp":1664790189146,"user_tz":-330,"elapsed":749,"user":{"displayName":"Jannie Sanjana Stephen","userId":"11861105524006828101"}}},"outputs":[],"source":["import torch.optim as optim\n","\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)"]},{"cell_type":"code","execution_count":133,"metadata":{"id":"OTG0F2U7iGZK","executionInfo":{"status":"ok","timestamp":1664790189568,"user_tz":-330,"elapsed":5,"user":{"displayName":"Jannie Sanjana Stephen","userId":"11861105524006828101"}}},"outputs":[],"source":["criterion = nn.BCEWithLogitsLoss()\n","\n","model = model.to(device)\n","criterion = criterion.to(device)"]},{"cell_type":"code","execution_count":134,"metadata":{"id":"YzTun-JNiGZK","executionInfo":{"status":"ok","timestamp":1664790190760,"user_tz":-330,"elapsed":7,"user":{"displayName":"Jannie Sanjana Stephen","userId":"11861105524006828101"}}},"outputs":[],"source":["def binary_accuracy(preds, y):\n","\n","    #round predictions to the closest integer\n","    rounded_preds = torch.round(torch.sigmoid(preds))\n","    correct = (rounded_preds == y).float() #convert into float for division \n","    acc = correct.sum() / len(correct)\n","    return acc"]},{"cell_type":"code","execution_count":135,"metadata":{"id":"KHQ2PRhnk3wb","executionInfo":{"status":"ok","timestamp":1664790192930,"user_tz":-330,"elapsed":16,"user":{"displayName":"Jannie Sanjana Stephen","userId":"11861105524006828101"}}},"outputs":[],"source":["from tqdm import tqdm"]},{"cell_type":"code","execution_count":136,"metadata":{"id":"L68FdNGziGZL","executionInfo":{"status":"ok","timestamp":1664790193973,"user_tz":-330,"elapsed":368,"user":{"displayName":"Jannie Sanjana Stephen","userId":"11861105524006828101"}}},"outputs":[],"source":["def train(model, iterator, optimizer, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.train()\n","    \n","    for batch in iterator:\n","        \n","        optimizer.zero_grad()\n","        \n","        predictions = model(batch.text).squeeze(1)\n","        \n","        loss = criterion(predictions, batch.label)\n","        \n","        acc = binary_accuracy(predictions, batch.label)\n","        \n","        loss.backward()\n","        \n","        optimizer.step()\n","        \n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"]},{"cell_type":"code","execution_count":137,"metadata":{"id":"j9SQuS0KiGZL","executionInfo":{"status":"ok","timestamp":1664790195928,"user_tz":-330,"elapsed":332,"user":{"displayName":"Jannie Sanjana Stephen","userId":"11861105524006828101"}}},"outputs":[],"source":["def evaluate(model, iterator, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.eval()\n","    \n","    with torch.no_grad():\n","    \n","        for batch in tqdm(iterator):\n","            \n","            predictions = model(batch.text).squeeze(1)\n","            \n","            loss = criterion(predictions, batch.label)\n","            \n","            acc = binary_accuracy(predictions, batch.label)\n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"]},{"cell_type":"code","execution_count":138,"metadata":{"id":"XUuz7hkwiGZL","executionInfo":{"status":"ok","timestamp":1664790198813,"user_tz":-330,"elapsed":336,"user":{"displayName":"Jannie Sanjana Stephen","userId":"11861105524006828101"}}},"outputs":[],"source":["import time\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"]},{"cell_type":"code","execution_count":139,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ctGpVoShiGZM","outputId":"98ab0ac2-d9bb-49cc-c26e-98f96d11a9ce","executionInfo":{"status":"ok","timestamp":1664790304870,"user_tz":-330,"elapsed":90180,"user":{"displayName":"Jannie Sanjana Stephen","userId":"11861105524006828101"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 118/118 [00:02<00:00, 54.70it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Epoch Time: 0m 16s\n","\tTrain Loss: 0.685 | Train Acc: 54.46%\n","\t Val. Loss: 0.622 |  Val. Acc: 66.87%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 118/118 [00:02<00:00, 47.43it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 02 | Epoch Time: 0m 17s\n","\tTrain Loss: 0.675 | Train Acc: 58.32%\n","\t Val. Loss: 0.655 |  Val. Acc: 58.71%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 118/118 [00:02<00:00, 53.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 03 | Epoch Time: 0m 17s\n","\tTrain Loss: 0.572 | Train Acc: 71.19%\n","\t Val. Loss: 0.706 |  Val. Acc: 59.26%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 118/118 [00:02<00:00, 50.33it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 04 | Epoch Time: 0m 18s\n","\tTrain Loss: 0.434 | Train Acc: 81.27%\n","\t Val. Loss: 0.374 |  Val. Acc: 84.94%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 118/118 [00:02<00:00, 46.45it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch: 05 | Epoch Time: 0m 18s\n","\tTrain Loss: 0.388 | Train Acc: 83.98%\n","\t Val. Loss: 0.378 |  Val. Acc: 84.58%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["N_EPOCHS = 5\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    \n","    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n","    \n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'Multilayer-model.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"]},{"cell_type":"code","execution_count":125,"metadata":{"id":"iEIJg2qsiGZM","executionInfo":{"status":"ok","timestamp":1664789909831,"user_tz":-330,"elapsed":4330,"user":{"displayName":"Jannie Sanjana Stephen","userId":"11861105524006828101"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c5569edf-1666-4771-d5e2-ee6839d4328d"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 391/391 [00:04<00:00, 89.02it/s]"]},{"output_type":"stream","name":"stdout","text":["LSTM model results\n","Test Loss: 0.615 | Test Acc: 66.78%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["model.load_state_dict(torch.load('LSTM-model.pt'))\n","\n","test_loss, test_acc = evaluate(model, test_iterator, criterion)\n","\n","print('LSTM model results')\n","print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"]},{"cell_type":"code","source":["model.load_state_dict(torch.load('Bidirectional-model.pt'))\n","\n","test_loss, test_acc = evaluate(model, test_iterator, criterion)\n","\n","print('Bidirectional RNN model results')\n","print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hXpJCkNjMMcI","executionInfo":{"status":"ok","timestamp":1664789796593,"user_tz":-330,"elapsed":14835,"user":{"displayName":"Jannie Sanjana Stephen","userId":"11861105524006828101"}},"outputId":"f55b7a5b-2aea-4291-b863-b9dbfa26d478"},"execution_count":111,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 391/391 [00:14<00:00, 26.96it/s]"]},{"output_type":"stream","name":"stdout","text":["Bidirectional RNN model results\n","Test Loss: 0.308 | Test Acc: 87.40%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["model.load_state_dict(torch.load('Multilayer-model.pt'))\n","\n","test_loss, test_acc = evaluate(model, test_iterator, criterion)\n","\n","print('Multilayer RNN model results')\n","print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QAY8IjbhOvUv","executionInfo":{"status":"ok","timestamp":1664790311999,"user_tz":-330,"elapsed":7169,"user":{"displayName":"Jannie Sanjana Stephen","userId":"11861105524006828101"}},"outputId":"f728916c-e950-46cd-a0ab-2d0a3e8090ae"},"execution_count":140,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 391/391 [00:07<00:00, 53.58it/s]"]},{"output_type":"stream","name":"stdout","text":["Multilayer RNN model results\n","Test Loss: 0.386 | Test Acc: 83.98%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["1. LSTM performs better than RNN. However the accuracy has to be much improved (using pre-processing steps)\n","2. ADAM performs better than SGD\n","3. The bidirectional RNN performs the best"],"metadata":{"id":"WssdpC4KO23S"}}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/SanVik2000/EE5179-Final/blob/main/Tutorial-7/Task_02.ipynb","timestamp":1664365563439}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":0}